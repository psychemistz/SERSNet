{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utils\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "## Classical Learner\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "class OrdinalClassifier():\n",
    "    \n",
    "    def __init__(self, clf):\n",
    "        self.clf = clf\n",
    "        self.clfs = {}\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.unique_class = np.sort(np.unique(y))\n",
    "        if self.unique_class.shape[0] > 2:\n",
    "            for i in range(self.unique_class.shape[0]-1):\n",
    "                # for each k - 1 ordinal value we fit a binary classification problem\n",
    "                binary_y = (y > self.unique_class[i]).astype(np.uint8)\n",
    "                clf = clone(self.clf)\n",
    "                clf.fit(X, binary_y)\n",
    "                self.clfs[i] = clf\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        clfs_predict = {k:self.clfs[k].predict_proba(X) for k in self.clfs}\n",
    "        predicted = []\n",
    "        for i,y in enumerate(self.unique_class):\n",
    "            if i == 0:\n",
    "                # V1 = 1 - Pr(y > V1)\n",
    "                predicted.append(1 - clfs_predict[y][:,1])\n",
    "            elif y in clfs_predict:\n",
    "                # Vi = Pr(y > Vi-1) - Pr(y > Vi)\n",
    "                 predicted.append(clfs_predict[y-1][:,1] - clfs_predict[y][:,1])\n",
    "            else:\n",
    "                # Vk = Pr(y > Vk-1)\n",
    "                predicted.append(clfs_predict[y-1][:,1])\n",
    "        return np.vstack(predicted).T\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dataset Parameter Setting\"\"\"\n",
    "\"\"\"Load Dataset\"\"\"\n",
    "r6g_dset = pd.read_csv(\"C:/Users/sypark/Desktop/Projects/w_MinSeok/1SERSNet/2data/_preprocessed/sersnet_devset.csv\")\n",
    "r6g_tset1 = pd.read_csv(\"C:/Users/sypark/Desktop/Projects/w_MinSeok/1SERSNet/2data/_preprocessed/sersnet_batch1_devset.csv\")\n",
    "r6g_tset2 = pd.read_csv(\"C:/Users/sypark/Desktop/Projects/w_MinSeok/1SERSNet/2data/_preprocessed/sersnet_devse_batch3_1.csv\")\n",
    "r6g_tset3 = pd.read_csv(\"C:/Users/sypark/Desktop/Projects/w_MinSeok/1SERSNet/2data/_preprocessed/sersnet_devse_batch3_2.csv\")\n",
    "r6g_tset4 = pd.read_csv(\"C:/Users/sypark/Desktop/Projects/w_MinSeok/1SERSNet/2data/_preprocessed/sersnet_devse_batch3_3.csv\")\n",
    "\n",
    "\"\"\"Set Output Path\"\"\"\n",
    "fileout = \"C:/Users/sypark/Desktop/Projects/w_MinSeok/1SERSNet/3results/R6G_Classification/raw_data/baseline_r6g_ordinal_model_output_dev_to_test.csv\"\n",
    "\n",
    "## Class Definition\n",
    "## 2: Conc. >= 10 mM  \n",
    "## 1: 10 uM <= Conc. < 10 mM\n",
    "## 0: Conc. < 10 uM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r6g_dset = r6g_dset.iloc[:,1:]\n",
    "r6g_tset1 = r6g_tset1.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "r6g_dset.loc[r6g_dset.label==2, 'label'] = int(3)\n",
    "r6g_dset.loc[r6g_dset.label==1, 'label'] = int(2)\n",
    "r6g_dset.loc[r6g_dset.label==3, 'label'] = int(1)\n",
    "dummy_label1 = r6g_dset[r6g_dset.label == 1]\n",
    "dummy_label1 = dummy_label1.iloc[0:1,:].reset_index(drop=True)\n",
    "dummy_label2 = r6g_dset[r6g_dset.label == 2]\n",
    "dummy_label2 = dummy_label2.iloc[0:1,:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "r6g_tset1.loc[r6g_tset1.label==1, 'label'] = int(2)\n",
    "r6g_tset3.loc[r6g_tset3.label==1, 'label'] = int(0)\n",
    "r6g_tset4.loc[r6g_tset4.label==1, 'label'] = int(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r6g_tset1 = pd.concat([r6g_tset1, dummy_label1]).reset_index(drop=True)\n",
    "r6g_tset2 = pd.concat([r6g_tset2, dummy_label2]).reset_index(drop=True)\n",
    "r6g_tset3 = pd.concat([r6g_tset3, dummy_label1, dummy_label2]).reset_index(drop=True)\n",
    "r6g_tset4 = pd.concat([r6g_tset4, dummy_label1, dummy_label2]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dset = r6g_dset.iloc[:, 1:].to_numpy(dtype='float32')\n",
    "X_tset1 = r6g_tset1.iloc[:, 1:].to_numpy(dtype='float32')\n",
    "X_tset2 = r6g_tset2.iloc[:, 1:].to_numpy(dtype='float32')\n",
    "X_tset3 = r6g_tset3.iloc[:, 1:].to_numpy(dtype='float32')\n",
    "X_tset4 = r6g_tset4.iloc[:, 1:].to_numpy(dtype='float32')\n",
    "\n",
    "y_dset = r6g_dset.iloc[:,0].to_numpy(dtype='int64') \n",
    "y_tset1 = r6g_tset1.iloc[:,0].to_numpy()            \n",
    "y_tset2 = r6g_tset2.iloc[:,0].to_numpy()\n",
    "y_tset3 = r6g_tset3.iloc[:,0].to_numpy()\n",
    "y_tset4 = r6g_tset4.iloc[:,0].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.4 train and test split for Classical Learner\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dset, y_dset, test_size = 0.2, \n",
    "                                                    random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classical Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learn Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(BernoulliNB())\n",
    "scaler.fit(X_train)\n",
    "X_train_sds = scaler.transform(X_train)\n",
    "X_test_sds = scaler.transform(X_test)\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 94   0   0]\n",
      " [ 91   1   0]\n",
      " [  0   0 114]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.697\n",
      "BACC: 0.67\n",
      "F1_micro: 0.697\n",
      "F1_macro: 0.565\n",
      "AUROC_OVR: 0.763\n",
      "AUROC_OVO: 0.755\n",
      "Precisio_micro: 0.697\n",
      "Precisio_macro: 0.836\n",
      "Recall_micro: 0.697\n",
      "Recall_macro: 0.67\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[500   0   0]\n",
      " [  1   0   0]\n",
      " [142  15 394]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.85\n",
      "BACC: 0.572\n",
      "F1_micro: 0.85\n",
      "F1_macro: 0.57\n",
      "AUROC_OVR: 0.901\n",
      "AUROC_OVO: 0.84\n",
      "Precisio_micro: 0.85\n",
      "Precisio_macro: 0.593\n",
      "Recall_micro: 0.85\n",
      "Recall_macro: 0.572\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset1)\n",
    "y_tset = y_tset1\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_tset1_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[499   1   0]\n",
      " [  0  90 410]\n",
      " [  0   0   1]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.589\n",
      "BACC: 0.726\n",
      "F1_micro: 0.589\n",
      "F1_macro: 0.436\n",
      "AUROC_OVR: 0.795\n",
      "AUROC_OVO: 0.795\n",
      "Precisio_micro: 0.589\n",
      "Precisio_macro: 0.664\n",
      "Recall_micro: 0.589\n",
      "Recall_macro: 0.726\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset2)\n",
    "y_tset = y_tset2\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_tset2_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[999   1   0]\n",
      " [  1   0   0]\n",
      " [  0   0   1]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.998\n",
      "BACC: 0.666\n",
      "F1_micro: 0.998\n",
      "F1_macro: 0.666\n",
      "AUROC_OVR: 0.916\n",
      "AUROC_OVO: 0.916\n",
      "Precisio_micro: 0.998\n",
      "Precisio_macro: 0.666\n",
      "Recall_micro: 0.998\n",
      "Recall_macro: 0.666\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset3)\n",
    "y_tset = y_tset3\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_tset3_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[999   1   0]\n",
      " [  1   0   0]\n",
      " [  0   0   1]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.998\n",
      "BACC: 0.666\n",
      "F1_micro: 0.998\n",
      "F1_macro: 0.666\n",
      "AUROC_OVR: 0.916\n",
      "AUROC_OVO: 0.916\n",
      "Precisio_micro: 0.998\n",
      "Precisio_macro: 0.666\n",
      "Recall_micro: 0.998\n",
      "Recall_macro: 0.666\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset4)\n",
    "y_tset = y_tset4\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_tset4_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "outF = open(fileout, \"w\")\n",
    "outF.write(\"Naive_Bayes, \")\n",
    "outF.write(\"ACC, BACC, F1_micro, F1_macro, AUROC_OVR, AUROC_OVO, Precision_micro, Precision_macro, Recall_micro, Recall_macro\\n\")\n",
    "outF.write('DevSet, ')\n",
    "outF.write(', '.join(map(str, NB_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet1, ')\n",
    "outF.write(', '.join(map(str, NB_tset1_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet2, ')\n",
    "outF.write(', '.join(map(str, NB_tset2_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet3, ')\n",
    "outF.write(', '.join(map(str, NB_tset3_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet4, ')\n",
    "outF.write(', '.join(map(str, NB_tset4_res)))\n",
    "outF.write('\\n')\n",
    "outF.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learn Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(DecisionTreeClassifier())\n",
    "scaler.fit(X_train)\n",
    "X_train_sds = scaler.transform(X_train)\n",
    "X_test_sds = scaler.transform(X_test)\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 94   0   0]\n",
      " [  1  91   0]\n",
      " [  0   0 114]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.997\n",
      "BACC: 0.996\n",
      "F1_micro: 0.997\n",
      "F1_macro: 0.996\n",
      "AUROC_OVR: 0.997\n",
      "AUROC_OVO: 0.997\n",
      "Precisio_micro: 0.997\n",
      "Precisio_macro: 0.996\n",
      "Recall_micro: 0.997\n",
      "Recall_macro: 0.996\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[500   0   0]\n",
      " [  0   1   0]\n",
      " [ 50 178 323]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.783\n",
      "BACC: 0.862\n",
      "F1_micro: 0.783\n",
      "F1_macro: 0.568\n",
      "AUROC_OVR: 0.888\n",
      "AUROC_OVO: 0.897\n",
      "Precisio_micro: 0.783\n",
      "Precisio_macro: 0.638\n",
      "Recall_micro: 0.783\n",
      "Recall_macro: 0.862\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset1)\n",
    "y_tset = y_tset1\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_tset1_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[485  15   0]\n",
      " [  0  91 409]\n",
      " [  0   0   1]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.576\n",
      "BACC: 0.717\n",
      "F1_micro: 0.576\n",
      "F1_macro: 0.43\n",
      "AUROC_OVR: 0.786\n",
      "AUROC_OVO: 0.788\n",
      "Precisio_micro: 0.576\n",
      "Precisio_macro: 0.62\n",
      "Recall_micro: 0.576\n",
      "Recall_macro: 0.717\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset2)\n",
    "y_tset = y_tset2\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_tset2_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[485 515   0]\n",
      " [  0   1   0]\n",
      " [  0   0   1]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.486\n",
      "BACC: 0.828\n",
      "F1_micro: 0.486\n",
      "F1_macro: 0.552\n",
      "AUROC_OVR: 0.828\n",
      "AUROC_OVO: 0.871\n",
      "Precisio_micro: 0.486\n",
      "Precisio_macro: 0.667\n",
      "Recall_micro: 0.486\n",
      "Recall_macro: 0.828\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset3)\n",
    "y_tset = y_tset3\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_tset3_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[485 515   0]\n",
      " [  0   1   0]\n",
      " [  0   0   1]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.486\n",
      "BACC: 0.828\n",
      "F1_micro: 0.486\n",
      "F1_macro: 0.552\n",
      "AUROC_OVR: 0.828\n",
      "AUROC_OVO: 0.871\n",
      "Precisio_micro: 0.486\n",
      "Precisio_macro: 0.667\n",
      "Recall_micro: 0.486\n",
      "Recall_macro: 0.828\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset4)\n",
    "y_tset = y_tset4\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_tset4_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "outF = open(fileout, \"a\")\n",
    "outF.write(\"Decision_Tree, \")\n",
    "outF.write(\"ACC, BACC, F1_micro, F1_macro, AUROC_OVR, AUROC_OVO, Precision_micro, Precision_macro, Recall_micro, Recall_macro\\n\")\n",
    "outF.write('DevSet, ')\n",
    "outF.write(', '.join(map(str, DT_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet1, ')\n",
    "outF.write(', '.join(map(str, DT_tset1_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet2, ')\n",
    "outF.write(', '.join(map(str, DT_tset2_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet3, ')\n",
    "outF.write(', '.join(map(str, DT_tset3_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet4, ')\n",
    "outF.write(', '.join(map(str, DT_tset4_res)))\n",
    "outF.write('\\n')\n",
    "outF.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "## Learn Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(LogisticRegression())\n",
    "scaler.fit(X_train)\n",
    "X_train_sds = scaler.transform(X_train)\n",
    "X_test_sds = scaler.transform(X_test)\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 94   0   0]\n",
      " [  0  92   0]\n",
      " [  0   0 114]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 1.0\n",
      "BACC: 1.0\n",
      "F1_micro: 1.0\n",
      "F1_macro: 1.0\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precisio_micro: 1.0\n",
      "Precisio_macro: 1.0\n",
      "Recall_micro: 1.0\n",
      "Recall_macro: 1.0\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[500   0   0]\n",
      " [  0   1   0]\n",
      " [  0 196 355]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.814\n",
      "BACC: 0.881\n",
      "F1_micro: 0.814\n",
      "F1_macro: 0.598\n",
      "AUROC_OVR: 0.952\n",
      "AUROC_OVO: 0.936\n",
      "Precisio_micro: 0.814\n",
      "Precisio_macro: 0.668\n",
      "Recall_micro: 0.814\n",
      "Recall_macro: 0.881\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset1)\n",
    "y_tset = y_tset1\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_tset1_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[500   0   0]\n",
      " [  0 422  78]\n",
      " [  0   0   1]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.922\n",
      "BACC: 0.948\n",
      "F1_micro: 0.922\n",
      "F1_macro: 0.647\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precisio_micro: 0.922\n",
      "Precisio_macro: 0.671\n",
      "Recall_micro: 0.922\n",
      "Recall_macro: 0.948\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset2)\n",
    "y_tset = y_tset2\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_tset2_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[500 500   0]\n",
      " [  0   1   0]\n",
      " [  0   0   1]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.501\n",
      "BACC: 0.833\n",
      "F1_micro: 0.501\n",
      "F1_macro: 0.557\n",
      "AUROC_OVR: 0.998\n",
      "AUROC_OVO: 0.999\n",
      "Precisio_micro: 0.501\n",
      "Precisio_macro: 0.667\n",
      "Recall_micro: 0.501\n",
      "Recall_macro: 0.833\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset3)\n",
    "y_tset = y_tset3\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_tset3_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[500 500   0]\n",
      " [  0   1   0]\n",
      " [  0   0   1]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.501\n",
      "BACC: 0.833\n",
      "F1_micro: 0.501\n",
      "F1_macro: 0.557\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precisio_micro: 0.501\n",
      "Precisio_macro: 0.667\n",
      "Recall_micro: 0.501\n",
      "Recall_macro: 0.833\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset4)\n",
    "y_tset = y_tset4\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_tset4_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "outF = open(fileout, \"a\")\n",
    "outF.write(\"Logistic_Regression, \")\n",
    "outF.write(\"ACC, BACC, F1_micro, F1_macro, AUROC_OVR, AUROC_OVO, Precision_micro, Precision_macro, Recall_micro, Recall_macro\\n\")\n",
    "outF.write('DevSet, ')\n",
    "outF.write(', '.join(map(str, LR_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet1, ')\n",
    "outF.write(', '.join(map(str, LR_tset1_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet2, ')\n",
    "outF.write(', '.join(map(str, LR_tset2_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet3, ')\n",
    "outF.write(', '.join(map(str, LR_tset3_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet4, ')\n",
    "outF.write(', '.join(map(str, LR_tset4_res)))\n",
    "outF.write('\\n')\n",
    "outF.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Logistic Regresssion CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learn Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(LogisticRegressionCV())\n",
    "scaler.fit(X_train)\n",
    "X_train_sds = scaler.transform(X_train)\n",
    "X_test_sds = scaler.transform(X_test)\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 94   0   0]\n",
      " [  0  92   0]\n",
      " [  0   0 114]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 1.0\n",
      "BACC: 1.0\n",
      "F1_micro: 1.0\n",
      "F1_macro: 1.0\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precisio_micro: 1.0\n",
      "Precisio_macro: 1.0\n",
      "Recall_micro: 1.0\n",
      "Recall_macro: 1.0\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_CV_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[500   0   0]\n",
      " [  0   1   0]\n",
      " [ 15 182 354]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.813\n",
      "BACC: 0.881\n",
      "F1_micro: 0.813\n",
      "F1_macro: 0.593\n",
      "AUROC_OVR: 0.954\n",
      "AUROC_OVO: 0.936\n",
      "Precisio_micro: 0.813\n",
      "Precisio_macro: 0.659\n",
      "Recall_micro: 0.813\n",
      "Recall_macro: 0.881\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset1)\n",
    "y_tset = y_tset1\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_CV_tset1_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[498   2   0]\n",
      " [  0 446  54]\n",
      " [  0   0   1]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.944\n",
      "BACC: 0.963\n",
      "F1_micro: 0.944\n",
      "F1_macro: 0.658\n",
      "AUROC_OVR: 0.998\n",
      "AUROC_OVO: 0.999\n",
      "Precisio_micro: 0.944\n",
      "Precisio_macro: 0.671\n",
      "Recall_micro: 0.944\n",
      "Recall_macro: 0.963\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset2)\n",
    "y_tset = y_tset2\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_CV_tset2_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[790 210   0]\n",
      " [  0   1   0]\n",
      " [  0   0   1]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.79\n",
      "BACC: 0.93\n",
      "F1_micro: 0.79\n",
      "F1_macro: 0.631\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precisio_micro: 0.79\n",
      "Precisio_macro: 0.668\n",
      "Recall_micro: 0.79\n",
      "Recall_macro: 0.93\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset3)\n",
    "y_tset = y_tset3\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_CV_tset3_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[985  15   0]\n",
      " [  0   1   0]\n",
      " [  0   0   1]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.985\n",
      "BACC: 0.995\n",
      "F1_micro: 0.985\n",
      "F1_macro: 0.703\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precisio_micro: 0.985\n",
      "Precisio_macro: 0.688\n",
      "Recall_micro: 0.985\n",
      "Recall_macro: 0.995\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset4)\n",
    "y_tset = y_tset4\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_CV_tset4_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "outF = open(fileout, \"a\")\n",
    "outF.write(\"Logistic_Regression_CV, \")\n",
    "outF.write(\"ACC, BACC, F1_micro, F1_macro, AUROC_OVR, AUROC_OVO, Precision_micro, Precision_macro, Recall_micro, Recall_macro\\n\")\n",
    "outF.write('DevSet, ')\n",
    "outF.write(', '.join(map(str, LR_CV_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet1, ')\n",
    "outF.write(', '.join(map(str, LR_CV_tset1_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet2, ')\n",
    "outF.write(', '.join(map(str, LR_CV_tset2_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet3, ')\n",
    "outF.write(', '.join(map(str, LR_CV_tset3_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet4, ')\n",
    "outF.write(', '.join(map(str, LR_CV_tset4_res)))\n",
    "outF.write('\\n')\n",
    "outF.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learn Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(MLPClassifier())\n",
    "scaler.fit(X_train)\n",
    "X_train_sds = scaler.transform(X_train)\n",
    "X_test_sds = scaler.transform(X_test)\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 94   0   0]\n",
      " [  0  92   0]\n",
      " [  0   0 114]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 1.0\n",
      "BACC: 1.0\n",
      "F1_micro: 1.0\n",
      "F1_macro: 1.0\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precisio_micro: 1.0\n",
      "Precisio_macro: 1.0\n",
      "Recall_micro: 1.0\n",
      "Recall_macro: 1.0\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[500   0   0]\n",
      " [  0   1   0]\n",
      " [ 27 130 394]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.851\n",
      "BACC: 0.905\n",
      "F1_micro: 0.851\n",
      "F1_macro: 0.608\n",
      "AUROC_OVR: 0.953\n",
      "AUROC_OVO: 0.935\n",
      "Precisio_micro: 0.851\n",
      "Precisio_macro: 0.652\n",
      "Recall_micro: 0.851\n",
      "Recall_macro: 0.905\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset1)\n",
    "y_tset = y_tset1\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_tset1_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[500   0   0]\n",
      " [ 70   0 430]\n",
      " [  0   0   1]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.5\n",
      "BACC: 0.667\n",
      "F1_micro: 0.5\n",
      "F1_macro: 0.313\n",
      "AUROC_OVR: 0.684\n",
      "AUROC_OVO: 0.684\n",
      "Precision_micro: 0.5\n",
      "Precision_macro: 0.293\n",
      "Recall_micro: 0.5\n",
      "Recall_macro: 0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset2)\n",
    "y_tset = y_tset2\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precision_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precision_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "MLP_tset2_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[500 500   0]\n",
      " [  0   1   0]\n",
      " [  0   0   1]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.501\n",
      "BACC: 0.833\n",
      "F1_micro: 0.501\n",
      "F1_macro: 0.557\n",
      "AUROC_OVR: 0.994\n",
      "AUROC_OVO: 0.996\n",
      "Precision_micro: 0.501\n",
      "Precision_macro: 0.667\n",
      "Recall_micro: 0.501\n",
      "Recall_macro: 0.833\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset3)\n",
    "y_tset = y_tset3\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precision_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precision_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_tset3_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[500 500   0]\n",
      " [  0   1   0]\n",
      " [  0   0   1]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.501\n",
      "BACC: 0.833\n",
      "F1_micro: 0.501\n",
      "F1_macro: 0.557\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precision_micro: 0.501\n",
      "Precision_macro: 0.667\n",
      "Recall_micro: 0.501\n",
      "Recall_macro: 0.833\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset4)\n",
    "y_tset = y_tset4\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precision_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precision_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_tset4_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "outF = open(fileout, \"a\")\n",
    "outF.write(\"MLP, \")\n",
    "outF.write(\"ACC, BACC, F1_micro, F1_macro, AUROC_OVR, AUROC_OVO, Precision_micro, Precision_macro, Recall_micro, Recall_macro\\n\")\n",
    "outF.write('DevSet, ')\n",
    "outF.write(', '.join(map(str, MLP_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet1, ')\n",
    "outF.write(', '.join(map(str, MLP_tset1_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet2, ')\n",
    "outF.write(', '.join(map(str, MLP_tset2_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet3, ')\n",
    "outF.write(', '.join(map(str, MLP_tset3_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet4, ')\n",
    "outF.write(', '.join(map(str, MLP_tset4_res)))\n",
    "outF.write('\\n')\n",
    "outF.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learn Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(RandomForestClassifier())\n",
    "scaler.fit(X_train)\n",
    "X_train_sds = scaler.transform(X_train)\n",
    "X_test_sds = scaler.transform(X_test)\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 93   1   0]\n",
      " [  0  92   0]\n",
      " [  0   0 114]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.997\n",
      "BACC: 0.996\n",
      "F1_micro: 0.997\n",
      "F1_macro: 0.996\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precisio_micro: 0.997\n",
      "Precisio_macro: 0.996\n",
      "Recall_micro: 0.997\n",
      "Recall_macro: 0.996\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[499   1   0]\n",
      " [  0   1   0]\n",
      " [ 38 156 357]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.815\n",
      "BACC: 0.882\n",
      "F1_micro: 0.815\n",
      "F1_macro: 0.587\n",
      "AUROC_OVR: 0.967\n",
      "AUROC_OVO: 0.967\n",
      "Precision_micro: 0.815\n",
      "Precision_macro: 0.645\n",
      "Recall_micro: 0.815\n",
      "Recall_macro: 0.882\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset1)\n",
    "y_tset = y_tset1\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precision_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precision_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_tset1_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[493   7   0]\n",
      " [  0 448  52]\n",
      " [  0   0   1]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.941\n",
      "BACC: 0.961\n",
      "F1_micro: 0.941\n",
      "F1_macro: 0.656\n",
      "AUROC_OVR: 0.997\n",
      "AUROC_OVO: 0.998\n",
      "Precision_micro: 0.941\n",
      "Precision_macro: 0.668\n",
      "Recall_micro: 0.941\n",
      "Recall_macro: 0.961\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset2)\n",
    "y_tset = y_tset2\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precision_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precision_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_tset2_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[493 507   0]\n",
      " [  0   1   0]\n",
      " [  0   0   1]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.494\n",
      "BACC: 0.831\n",
      "F1_micro: 0.494\n",
      "F1_macro: 0.555\n",
      "AUROC_OVR: 0.835\n",
      "AUROC_OVO: 0.876\n",
      "Precision_micro: 0.494\n",
      "Precision_macro: 0.667\n",
      "Recall_micro: 0.494\n",
      "Recall_macro: 0.831\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset3)\n",
    "y_tset = y_tset3\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precision_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precision_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_tset3_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[493 507   0]\n",
      " [  0   1   0]\n",
      " [  0   0   1]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.494\n",
      "BACC: 0.831\n",
      "F1_micro: 0.494\n",
      "F1_macro: 0.555\n",
      "AUROC_OVR: 0.847\n",
      "AUROC_OVO: 0.885\n",
      "Precision_micro: 0.494\n",
      "Precision_macro: 0.667\n",
      "Recall_micro: 0.494\n",
      "Recall_macro: 0.831\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset4)\n",
    "y_tset = y_tset4\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precision_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precision_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_tset4_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "outF = open(fileout, \"a\")\n",
    "outF.write(\"Random_Forest, \")\n",
    "outF.write(\"ACC, BACC, F1_micro, F1_macro, AUROC_OVR, AUROC_OVO, Precision_micro, Precision_macro, Recall_micro, Recall_macro\\n\")\n",
    "outF.write('DevSet, ')\n",
    "outF.write(', '.join(map(str, RF_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet1, ')\n",
    "outF.write(', '.join(map(str, RF_tset1_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet2, ')\n",
    "outF.write(', '.join(map(str, RF_tset2_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet3, ')\n",
    "outF.write(', '.join(map(str, RF_tset3_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet4, ')\n",
    "outF.write(', '.join(map(str, RF_tset4_res)))\n",
    "outF.write('\\n')\n",
    "outF.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learn Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(SVC(kernel = 'linear', probability=True))\n",
    "scaler.fit(X_train)\n",
    "X_train_sds = scaler.transform(X_train)\n",
    "X_test_sds = scaler.transform(X_test)\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 94   0   0]\n",
      " [  0  92   0]\n",
      " [  0   0 114]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 1.0\n",
      "BACC: 1.0\n",
      "F1_micro: 1.0\n",
      "F1_macro: 1.0\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precisio_micro: 1.0\n",
      "Precisio_macro: 1.0\n",
      "Recall_micro: 1.0\n",
      "Recall_macro: 1.0\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinSVM_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[500   0   0]\n",
      " [  0   1   0]\n",
      " [  0 197 354]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.813\n",
      "BACC: 0.881\n",
      "F1_micro: 0.813\n",
      "F1_macro: 0.597\n",
      "AUROC_OVR: 0.922\n",
      "AUROC_OVO: 0.91\n",
      "Precision_micro: 0.813\n",
      "Precision_macro: 0.668\n",
      "Recall_micro: 0.813\n",
      "Recall_macro: 0.881\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset1)\n",
    "y_tset = y_tset1\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precision_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precision_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinSVM_tset1_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[500   0   0]\n",
      " [  0 432  68]\n",
      " [  0   0   1]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.932\n",
      "BACC: 0.955\n",
      "F1_micro: 0.932\n",
      "F1_macro: 0.652\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precision_micro: 0.932\n",
      "Precision_macro: 0.671\n",
      "Recall_micro: 0.932\n",
      "Recall_macro: 0.955\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset2)\n",
    "y_tset = y_tset2\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precision_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precision_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinSVM_tset2_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[500 500   0]\n",
      " [  0   1   0]\n",
      " [  0   0   1]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.501\n",
      "BACC: 0.833\n",
      "F1_micro: 0.501\n",
      "F1_macro: 0.557\n",
      "AUROC_OVR: 0.76\n",
      "AUROC_OVO: 0.844\n",
      "Precision_micro: 0.501\n",
      "Precision_macro: 0.667\n",
      "Recall_micro: 0.501\n",
      "Recall_macro: 0.833\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset3)\n",
    "y_tset = y_tset3\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precision_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precision_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinSVM_tset3_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[500 500   0]\n",
      " [  0   1   0]\n",
      " [  0   0   1]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.501\n",
      "BACC: 0.833\n",
      "F1_micro: 0.501\n",
      "F1_macro: 0.557\n",
      "AUROC_OVR: 0.828\n",
      "AUROC_OVO: 0.911\n",
      "Precision_micro: 0.501\n",
      "Precision_macro: 0.667\n",
      "Recall_micro: 0.501\n",
      "Recall_macro: 0.833\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset4)\n",
    "y_tset = y_tset4\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precision_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precision_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinSVM_tset4_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "outF = open(fileout, \"a\")\n",
    "outF.write(\"Linear_SVM, \")\n",
    "outF.write(\"ACC, BACC, F1_micro, F1_macro, AUROC_OVR, AUROC_OVO, Precision_micro, Precision_macro, Recall_micro, Recall_macro\\n\")\n",
    "outF.write('DevSet, ')\n",
    "outF.write(', '.join(map(str, LinSVM_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet1, ')\n",
    "outF.write(', '.join(map(str, LinSVM_tset1_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet2, ')\n",
    "outF.write(', '.join(map(str, LinSVM_tset2_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet3, ')\n",
    "outF.write(', '.join(map(str, LinSVM_tset3_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet4, ')\n",
    "outF.write(', '.join(map(str, LinSVM_tset4_res)))\n",
    "outF.write('\\n')\n",
    "outF.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 RBF SVM (Nonlinear SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learn Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(SVC(kernel = 'rbf', probability=True))\n",
    "scaler.fit(X_train)\n",
    "X_train_sds = scaler.transform(X_train)\n",
    "X_test_sds = scaler.transform(X_test)\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 94   0   0]\n",
      " [  0  92   0]\n",
      " [  0   0 114]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 1.0\n",
      "BACC: 1.0\n",
      "F1_micro: 1.0\n",
      "F1_macro: 1.0\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precisio_micro: 1.0\n",
      "Precisio_macro: 1.0\n",
      "Recall_micro: 1.0\n",
      "Recall_macro: 1.0\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "RBFSVM_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[499   1   0]\n",
      " [  0   1   0]\n",
      " [ 43 202 306]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.766\n",
      "BACC: 0.851\n",
      "F1_micro: 0.766\n",
      "F1_macro: 0.561\n",
      "AUROC_OVR: 0.96\n",
      "AUROC_OVO: 0.954\n",
      "Precision_micro: 0.766\n",
      "Precision_macro: 0.642\n",
      "Recall_micro: 0.766\n",
      "Recall_macro: 0.851\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset1)\n",
    "y_tset = y_tset1\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precision_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precision_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "RBFSVM_tset1_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[498   2   0]\n",
      " [402  27  71]\n",
      " [  0   0   1]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.525\n",
      "BACC: 0.683\n",
      "F1_micro: 0.525\n",
      "F1_macro: 0.28\n",
      "AUROC_OVR: 0.857\n",
      "AUROC_OVO: 0.854\n",
      "Precision_micro: 0.525\n",
      "Precision_macro: 0.499\n",
      "Recall_micro: 0.525\n",
      "Recall_macro: 0.683\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset2)\n",
    "y_tset = y_tset2\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precision_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precision_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "RBFSVM_tset2_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[498 502   0]\n",
      " [  0   1   0]\n",
      " [  0   0   1]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.499\n",
      "BACC: 0.833\n",
      "F1_micro: 0.499\n",
      "F1_macro: 0.556\n",
      "AUROC_OVR: 0.69\n",
      "AUROC_OVO: 0.762\n",
      "Precision_micro: 0.499\n",
      "Precision_macro: 0.667\n",
      "Recall_micro: 0.499\n",
      "Recall_macro: 0.833\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset3)\n",
    "y_tset = y_tset3\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precision_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precision_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "RBFSVM_tset3_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[498 502   0]\n",
      " [  0   1   0]\n",
      " [  0   0   1]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.499\n",
      "BACC: 0.833\n",
      "F1_micro: 0.499\n",
      "F1_macro: 0.556\n",
      "AUROC_OVR: 0.826\n",
      "AUROC_OVO: 0.829\n",
      "Precision_micro: 0.499\n",
      "Precision_macro: 0.667\n",
      "Recall_micro: 0.499\n",
      "Recall_macro: 0.833\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset4)\n",
    "y_tset = y_tset4\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precision_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precision_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "RBFSVM_tset4_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "outF = open(fileout, \"a\")\n",
    "outF.write(\"RBF_SVM, \")\n",
    "outF.write(\"ACC, BACC, F1_micro, F1_macro, AUROC_OVR, AUROC_OVO, Precision_micro, Precision_macro, Recall_micro, Recall_macro\\n\")\n",
    "outF.write('DevSet, ')\n",
    "outF.write(', '.join(map(str, RBFSVM_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet1, ')\n",
    "outF.write(', '.join(map(str, RBFSVM_tset1_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet2, ')\n",
    "outF.write(', '.join(map(str, RBFSVM_tset2_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet3, ')\n",
    "outF.write(', '.join(map(str, RBFSVM_tset3_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('IndSet4, ')\n",
    "outF.write(', '.join(map(str, RBFSVM_tset4_res)))\n",
    "outF.write('\\n')\n",
    "outF.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
