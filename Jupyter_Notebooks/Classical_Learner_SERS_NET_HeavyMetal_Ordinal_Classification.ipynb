{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utils\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "## Classical Learner\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "class OrdinalClassifier():\n",
    "    \n",
    "    def __init__(self, clf):\n",
    "        self.clf = clf\n",
    "        self.clfs = {}\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.unique_class = np.sort(np.unique(y))\n",
    "        if self.unique_class.shape[0] > 2:\n",
    "            for i in range(self.unique_class.shape[0]-1):\n",
    "                # for each k - 1 ordinal value we fit a binary classification problem\n",
    "                binary_y = (y > self.unique_class[i]).astype(np.uint8)\n",
    "                clf = clone(self.clf)\n",
    "                clf.fit(X, binary_y)\n",
    "                self.clfs[i] = clf\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        clfs_predict = {k:self.clfs[k].predict_proba(X) for k in self.clfs}\n",
    "        predicted = []\n",
    "        for i,y in enumerate(self.unique_class):\n",
    "            if i == 0:\n",
    "                # V1 = 1 - Pr(y > V1)\n",
    "                predicted.append(1 - clfs_predict[y][:,1])\n",
    "            elif y in clfs_predict:\n",
    "                # Vi = Pr(y > Vi-1) - Pr(y > Vi)\n",
    "                 predicted.append(clfs_predict[y-1][:,1] - clfs_predict[y][:,1])\n",
    "            else:\n",
    "                # Vk = Pr(y > Vk-1)\n",
    "                predicted.append(clfs_predict[y-1][:,1])\n",
    "        return np.vstack(predicted).T\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dataset Parameter Setting\"\"\"\n",
    "\"\"\"Load Dataset\"\"\"\n",
    "cdcl2_b1_dset = pd.read_csv(\"C:/Users/sypark/Desktop/Projects/w_MinSeok/1SERSNet/2data/_preprocessed/sersnet_cdcl2_b1_bn_bl_corrected.csv\")\n",
    "cdcl2_b2_dset = pd.read_csv(\"C:/Users/sypark/Desktop/Projects/w_MinSeok/1SERSNet/2data/_preprocessed/sersnet_cdcl2_b2_bn_bl_corrected.csv\")\n",
    "\n",
    "pbcl2_b1_dset = pd.read_csv(\"C:/Users/sypark/Desktop/Projects/w_MinSeok/1SERSNet/2data/_preprocessed/sersnet_pbcl2_b1_bn_bl_corrected.csv\")\n",
    "pbcl2_b2_dset = pd.read_csv(\"C:/Users/sypark/Desktop/Projects/w_MinSeok/1SERSNet/2data/_preprocessed/sersnet_pbcl2_b2_bn_bl_corrected.csv\")\n",
    "\n",
    "pbno32_b1_dset = pd.read_csv(\"C:/Users/sypark/Desktop/Projects/w_MinSeok/1SERSNet/2data/_preprocessed/sersnet_pbno32_b1_bn_bl_corrected.csv\")\n",
    "pbno32_b2_dset = pd.read_csv(\"C:/Users/sypark/Desktop/Projects/w_MinSeok/1SERSNet/2data/_preprocessed/sersnet_pbno32_b2_bn_bl_corrected.csv\")\n",
    "\n",
    "\n",
    "\"\"\"Set Output Path\"\"\"\n",
    "fileout = \"C:/Users/sypark/Desktop/Projects/w_MinSeok/1SERSNet/3results/HM_Classification/raw_data/baseline_ordinal_hm_model_output_b2_to_b1_bn_bl_corrected.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdcl2_b1_dset = pd.concat([pd.DataFrame(cdcl2_b1_dset.iloc[:, 3]).rename(columns={'Concentration_uM': 'label'}), \n",
    "                           cdcl2_b1_dset.iloc[:, 5:]], axis=1)\n",
    "cdcl2_b2_dset = pd.concat([pd.DataFrame(cdcl2_b2_dset.iloc[:, 3]).rename(columns={'Concentration_uM': 'label'}), \n",
    "                           cdcl2_b2_dset.iloc[:, 5:]], axis=1)\n",
    "cdcl2_all_dset = pd.concat([cdcl2_b1_dset, cdcl2_b2_dset], axis=0).reset_index(drop=True)\n",
    "\n",
    "pbcl2_b1_dset = pd.concat([pd.DataFrame(pbcl2_b1_dset.iloc[:, 3]).rename(columns={'Concentration_uM': 'label'}), \n",
    "                           pbcl2_b1_dset.iloc[:, 5:]], axis=1)\n",
    "pbcl2_b2_dset = pd.concat([pd.DataFrame(pbcl2_b2_dset.iloc[:, 3]).rename(columns={'Concentration_uM': 'label'}), \n",
    "                           pbcl2_b2_dset.iloc[:, 5:]], axis=1)\n",
    "pbcl2_all_dset = pd.concat([pbcl2_b1_dset, pbcl2_b2_dset], axis=0).reset_index(drop=True)\n",
    "\n",
    "pbno32_b1_dset = pd.concat([pd.DataFrame(pbno32_b1_dset.iloc[:, 3]).rename(columns={'Concentration_uM': 'label'}), \n",
    "                            pbno32_b1_dset.iloc[:, 5:]], axis=1)\n",
    "pbno32_b2_dset = pd.concat([pd.DataFrame(pbno32_b2_dset.iloc[:, 3]).rename(columns={'Concentration_uM': 'label'}),\n",
    "                            pbno32_b2_dset.iloc[:, 5:]], axis=1)\n",
    "pbno32_all_dset = pd.concat([pbno32_b1_dset, pbno32_b2_dset], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_all_dset = pd.concat([cdcl2_all_dset, pbcl2_all_dset, pbno32_all_dset], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get Label Encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(hm_all_dset.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CdCl2 Dataset\n",
    "tmp1 = le.transform(cdcl2_b1_dset.iloc[:,0])\n",
    "tmp2 = le.transform(cdcl2_b2_dset.iloc[:,0])\n",
    "\n",
    "cdcl2_b1_dset = pd.concat([pd.DataFrame(tmp1).rename(columns={0:'label'}), cdcl2_b1_dset.iloc[:,1:]], axis=1)\n",
    "cdcl2_b2_dset = pd.concat([pd.DataFrame(tmp2).rename(columns={0:'label'}), cdcl2_b2_dset.iloc[:,1:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 3]\n",
      "[0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(cdcl2_b1_dset.label))\n",
    "print(np.unique(cdcl2_b2_dset.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pbcl2 Dataset\n",
    "tmp1 = le.transform(pbcl2_b1_dset.iloc[:,0])\n",
    "tmp2 = le.transform(pbcl2_b2_dset.iloc[:,0])\n",
    "\n",
    "pbcl2_b1_dset = pd.concat([pd.DataFrame(tmp1).rename(columns={0:'label'}), pbcl2_b1_dset.iloc[:,1:]], axis=1)\n",
    "pbcl2_b2_dset = pd.concat([pd.DataFrame(tmp2).rename(columns={0:'label'}), pbcl2_b2_dset.iloc[:,1:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "[0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(pbcl2_b1_dset.label))\n",
    "print(np.unique(pbcl2_b2_dset.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pb(NO3)2 Dataset\n",
    "tmp1 = le.transform(pbno32_b1_dset.iloc[:,0])\n",
    "tmp2 = le.transform(pbno32_b2_dset.iloc[:,0])\n",
    "\n",
    "pbno32_b1_dset = pd.concat([pd.DataFrame(tmp1).rename(columns={0:'label'}), pbno32_b1_dset.iloc[:,1:]], axis=1)\n",
    "pbno32_b2_dset = pd.concat([pd.DataFrame(tmp2).rename(columns={0:'label'}), pbno32_b2_dset.iloc[:,1:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "[0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(pbno32_b1_dset.label))\n",
    "print(np.unique(pbno32_b2_dset.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Train and Test Sets\n",
    "cdcl2_train_set = cdcl2_b2_dset\n",
    "tmp_dt = cdcl2_b2_dset[cdcl2_b2_dset.label==1]\n",
    "cdcl2_test_set = pd.concat([cdcl2_b1_dset, tmp_dt.iloc[0:1,:]], axis=0).reset_index(drop=True)\n",
    "\n",
    "pbcl2_train_set = pbcl2_b2_dset\n",
    "tmp_dt = pbcl2_b2_dset[pbcl2_b2_dset.label==0]\n",
    "pbcl2_test_set = pd.concat([pbcl2_b1_dset, tmp_dt.iloc[0:1,:]], axis=0).reset_index(drop=True)\n",
    "\n",
    "pbno32_train_set = pbno32_b2_dset\n",
    "tmp_dt = pbno32_b2_dset[pbno32_b2_dset.label==0]\n",
    "pbno32_test_set = pd.concat([pbno32_b1_dset, tmp_dt.iloc[0:1,:]], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdcl2_train_set = cdcl2_train_set.iloc[:,:-1]\n",
    "cdcl2_test_set = cdcl2_test_set.iloc[:,:-1]\n",
    "pbcl2_train_set = pbcl2_train_set.iloc[:,:-1]\n",
    "pbcl2_test_set = pbcl2_test_set.iloc[:,:-1]\n",
    "pbno32_train_set = pbno32_train_set.iloc[:,:-1]\n",
    "pbno32_test_set = pbno32_test_set.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cdcl2_dset = cdcl2_train_set.iloc[:, 1:].to_numpy(dtype='float32')\n",
    "X_cdcl2_tset = cdcl2_test_set.iloc[:, 1:].to_numpy(dtype='float32')\n",
    "y_cdcl2_dset = cdcl2_train_set.iloc[:,0].to_numpy(dtype='int64') \n",
    "y_cdcl2_tset = cdcl2_test_set.iloc[:,0].to_numpy(dtype='int64') \n",
    "\n",
    "\n",
    "X_pbcl2_dset = pbcl2_train_set.iloc[:, 1:].to_numpy(dtype='float32')\n",
    "X_pbcl2_tset = pbcl2_test_set.iloc[:, 1:].to_numpy(dtype='float32')\n",
    "y_pbcl2_dset = pbcl2_train_set.iloc[:,0].to_numpy(dtype='int64') \n",
    "y_pbcl2_tset = pbcl2_test_set.iloc[:,0].to_numpy(dtype='int64') \n",
    "\n",
    "X_pbno32_dset = pbno32_train_set.iloc[:, 1:].to_numpy(dtype='float32')\n",
    "X_pbno32_tset = pbno32_test_set.iloc[:, 1:].to_numpy(dtype='float32')\n",
    "y_pbno32_dset = pbno32_train_set.iloc[:,0].to_numpy(dtype='int64') \n",
    "y_pbno32_tset = pbno32_test_set.iloc[:,0].to_numpy(dtype='int64') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.4 train and test split for Classical Learner\n",
    "X_cdcl2_train, X_cdcl2_test, y_cdcl2_train, y_cdcl2_test = train_test_split(X_cdcl2_dset, y_cdcl2_dset, test_size = 0.2, \n",
    "                                                    random_state=123)\n",
    "\n",
    "X_pbcl2_train, X_pbcl2_test, y_pbcl2_train, y_pbcl2_test = train_test_split(X_pbcl2_dset, y_pbcl2_dset, test_size = 0.2, \n",
    "                                                    random_state=123)\n",
    "\n",
    "X_pbno32_train, X_pbno32_test, y_pbno32_train, y_pbno32_test = train_test_split(X_pbno32_dset, y_pbno32_dset, test_size = 0.2, \n",
    "                                                    random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classical Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learn Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(BernoulliNB())\n",
    "scaler.fit(X_cdcl2_train)\n",
    "X_train_sds = scaler.transform(X_cdcl2_train)\n",
    "X_test_sds = scaler.transform(X_cdcl2_test)\n",
    "y_train = y_cdcl2_train\n",
    "y_test = y_cdcl2_test\n",
    "X_tset = X_cdcl2_tset\n",
    "y_tset = y_cdcl2_tset\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[86 14  4  0]\n",
      " [17 72  0  0]\n",
      " [13 92  1  0]\n",
      " [ 0  8  7 86]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.612\n",
      "BACC: 0.624\n",
      "F1_micro: 0.612\n",
      "F1_macro: 0.561\n",
      "AUROC_OVR: 0.806\n",
      "AUROC_OVO: 0.805\n",
      "Precisio_micro: 0.612\n",
      "Precisio_macro: 0.553\n",
      "Recall_micro: 0.612\n",
      "Recall_macro: 0.624\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_cdcl2_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 18  98   6 378]\n",
      " [  0   1   0   0]\n",
      " [150   0   0 350]\n",
      " [500   0   0   0]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.013\n",
      "BACC: 0.259\n",
      "F1_micro: 0.013\n",
      "F1_macro: 0.013\n",
      "AUROC_OVR: 0.433\n",
      "AUROC_OVO: 0.454\n",
      "Precisio_micro: 0.013\n",
      "Precisio_macro: 0.009\n",
      "Recall_micro: 0.013\n",
      "Recall_macro: 0.259\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset)\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_cdcl2_tset_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learn Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(BernoulliNB())\n",
    "scaler.fit(X_cdcl2_train)\n",
    "X_train_sds = scaler.transform(X_pbcl2_train)\n",
    "X_test_sds = scaler.transform(X_pbcl2_test)\n",
    "y_train = y_pbcl2_train\n",
    "y_test = y_pbcl2_test\n",
    "X_tset = X_pbcl2_tset\n",
    "y_tset = y_pbcl2_tset\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 89   0   0   0]\n",
      " [ 19  78   1   3]\n",
      " [  0   2 104   0]\n",
      " [  0   0   0 104]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.938\n",
      "BACC: 0.938\n",
      "F1_micro: 0.938\n",
      "F1_macro: 0.934\n",
      "AUROC_OVR: 0.986\n",
      "AUROC_OVO: 0.986\n",
      "Precisio_micro: 0.938\n",
      "Precisio_macro: 0.94\n",
      "Recall_micro: 0.938\n",
      "Recall_macro: 0.938\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_pbcl2_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[  1   0   0   0]\n",
      " [395 100   3   2]\n",
      " [  4 439  57   0]\n",
      " [  0   0   0 500]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.438\n",
      "BACC: 0.578\n",
      "F1_micro: 0.438\n",
      "F1_macro: 0.35\n",
      "AUROC_OVR: 0.843\n",
      "AUROC_OVO: 0.871\n",
      "Precisio_micro: 0.438\n",
      "Precisio_macro: 0.534\n",
      "Recall_micro: 0.438\n",
      "Recall_macro: 0.578\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset)\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_pbcl2_tset_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learn Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(BernoulliNB())\n",
    "scaler.fit(X_pbno32_train)\n",
    "X_train_sds = scaler.transform(X_pbno32_train)\n",
    "X_test_sds = scaler.transform(X_pbno32_test)\n",
    "y_train = y_pbno32_train\n",
    "y_test = y_pbno32_test\n",
    "X_tset = X_pbno32_tset\n",
    "y_tset = y_pbno32_tset\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 81   8   0   0]\n",
      " [  2 101   0   1]\n",
      " [ 18   1  87   0]\n",
      " [  0   0   0 101]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.925\n",
      "BACC: 0.926\n",
      "F1_micro: 0.925\n",
      "F1_macro: 0.923\n",
      "AUROC_OVR: 0.975\n",
      "AUROC_OVO: 0.975\n",
      "Precisio_micro: 0.925\n",
      "Precisio_macro: 0.928\n",
      "Recall_micro: 0.925\n",
      "Recall_macro: 0.926\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_pbno32_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[  1   0   0   0]\n",
      " [431  64   5   0]\n",
      " [  8 158 334   0]\n",
      " [  0   0 396 104]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.335\n",
      "BACC: 0.501\n",
      "F1_micro: 0.335\n",
      "F1_macro: 0.267\n",
      "AUROC_OVR: 0.628\n",
      "AUROC_OVO: 0.662\n",
      "Precisio_micro: 0.335\n",
      "Precisio_macro: 0.436\n",
      "Recall_micro: 0.335\n",
      "Recall_macro: 0.501\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset)\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_pbno32_tset_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "outF = open(fileout, \"w\")\n",
    "outF.write(\"Naive_Bayes, \")\n",
    "outF.write(\"ACC, BACC, F1_micro, F1_macro, AUROC_OVR, AUROC_OVO, Precision_micro, Precision_macro, Recall_micro, Recall_macro\\n\")\n",
    "outF.write('Cdcl2 DevSet, ')\n",
    "outF.write(', '.join(map(str, NB_cdcl2_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Cdcl2 IndSet, ')\n",
    "outF.write(', '.join(map(str, NB_cdcl2_tset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pbcl2 DevSet, ')\n",
    "outF.write(', '.join(map(str, NB_pbcl2_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pbcl2 IndSet, ')\n",
    "outF.write(', '.join(map(str, NB_pbcl2_tset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pb(No3)2 DevSet, ')\n",
    "outF.write(', '.join(map(str, NB_pbno32_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pb(No3)2 IndSet, ')\n",
    "outF.write(', '.join(map(str, NB_pbno32_tset_res)))\n",
    "outF.write('\\n')\n",
    "outF.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cdcl2 Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(DecisionTreeClassifier())\n",
    "scaler.fit(X_cdcl2_train)\n",
    "X_train_sds = scaler.transform(X_cdcl2_train)\n",
    "X_test_sds = scaler.transform(X_cdcl2_test)\n",
    "y_train = y_cdcl2_train\n",
    "y_test = y_cdcl2_test\n",
    "X_tset = X_cdcl2_tset\n",
    "y_tset = y_cdcl2_tset\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[104   0   0   0]\n",
      " [  2  87   0   0]\n",
      " [  0   2 102   2]\n",
      " [  0   1   0 100]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.982\n",
      "BACC: 0.982\n",
      "F1_micro: 0.982\n",
      "F1_macro: 0.982\n",
      "AUROC_OVR: 0.989\n",
      "AUROC_OVO: 0.989\n",
      "Precisio_micro: 0.982\n",
      "Precisio_macro: 0.982\n",
      "Recall_micro: 0.982\n",
      "Recall_macro: 0.982\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_cdcl2_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[140 171  32 157]\n",
      " [  0   1   0   0]\n",
      " [123  59  28 290]\n",
      " [181  17  74 228]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.264\n",
      "BACC: 0.448\n",
      "F1_micro: 0.264\n",
      "F1_macro: 0.195\n",
      "AUROC_OVR: 0.607\n",
      "AUROC_OVO: 0.649\n",
      "Precisio_micro: 0.264\n",
      "Precisio_macro: 0.217\n",
      "Recall_micro: 0.264\n",
      "Recall_macro: 0.448\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset)\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_cdcl2_tset_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pbcl2 Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(DecisionTreeClassifier())\n",
    "scaler.fit(X_cdcl2_train)\n",
    "X_train_sds = scaler.transform(X_pbcl2_train)\n",
    "X_test_sds = scaler.transform(X_pbcl2_test)\n",
    "y_train = y_pbcl2_train\n",
    "y_test = y_pbcl2_test\n",
    "X_tset = X_pbcl2_tset\n",
    "y_tset = y_pbcl2_tset\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 89   0   0   0]\n",
      " [  0  99   2   0]\n",
      " [  0   1 105   0]\n",
      " [  0   0   0 104]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.992\n",
      "BACC: 0.993\n",
      "F1_micro: 0.992\n",
      "F1_macro: 0.993\n",
      "AUROC_OVR: 0.995\n",
      "AUROC_OVO: 0.995\n",
      "Precisio_micro: 0.992\n",
      "Precisio_macro: 0.993\n",
      "Recall_micro: 0.992\n",
      "Recall_macro: 0.993\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_pbcl2_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[  1   0   0   0]\n",
      " [289 211   0   0]\n",
      " [181 186 133   0]\n",
      " [  0  21   3 476]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.547\n",
      "BACC: 0.66\n",
      "F1_micro: 0.547\n",
      "F1_macro: 0.464\n",
      "AUROC_OVR: 0.798\n",
      "AUROC_OVO: 0.804\n",
      "Precisio_micro: 0.547\n",
      "Precisio_macro: 0.621\n",
      "Recall_micro: 0.547\n",
      "Recall_macro: 0.66\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset)\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_pbcl2_tset_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learn Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(DecisionTreeClassifier())\n",
    "scaler.fit(X_pbno32_train)\n",
    "X_train_sds = scaler.transform(X_pbno32_train)\n",
    "X_test_sds = scaler.transform(X_pbno32_test)\n",
    "y_train = y_pbno32_train\n",
    "y_test = y_pbno32_test\n",
    "X_tset = X_pbno32_tset\n",
    "y_tset = y_pbno32_tset\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 88   1   0   0]\n",
      " [  0 103   1   0]\n",
      " [  1   1 104   0]\n",
      " [  0   0   0 101]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.99\n",
      "BACC: 0.99\n",
      "F1_micro: 0.99\n",
      "F1_macro: 0.99\n",
      "AUROC_OVR: 0.994\n",
      "AUROC_OVO: 0.994\n",
      "Precisio_micro: 0.99\n",
      "Precisio_macro: 0.99\n",
      "Recall_micro: 0.99\n",
      "Recall_macro: 0.99\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_pbno32_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[  1   0   0   0]\n",
      " [303  62 135   0]\n",
      " [ 19 145 161 175]\n",
      " [  1   0 336 163]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.258\n",
      "BACC: 0.443\n",
      "F1_micro: 0.258\n",
      "F1_macro: 0.214\n",
      "AUROC_OVR: 0.505\n",
      "AUROC_OVO: 0.543\n",
      "Precisio_micro: 0.258\n",
      "Precisio_macro: 0.26\n",
      "Recall_micro: 0.258\n",
      "Recall_macro: 0.443\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset)\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_pbno32_tset_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "outF = open(fileout, \"a\")\n",
    "outF.write(\"Decision_Tree, \")\n",
    "outF.write(\"ACC, BACC, F1_micro, F1_macro, AUROC_OVR, AUROC_OVO, Precision_micro, Precision_macro, Recall_micro, Recall_macro\\n\")\n",
    "outF.write('Cdcl2 DevSet, ')\n",
    "outF.write(', '.join(map(str, DT_cdcl2_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Cdcl2 IndSet, ')\n",
    "outF.write(', '.join(map(str, DT_cdcl2_tset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pbcl2 DevSet, ')\n",
    "outF.write(', '.join(map(str, DT_pbcl2_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pbcl2 IndSet, ')\n",
    "outF.write(', '.join(map(str, DT_pbcl2_tset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pb(No3)2 DevSet, ')\n",
    "outF.write(', '.join(map(str, DT_pbno32_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pb(No3)2 IndSet, ')\n",
    "outF.write(', '.join(map(str, DT_pbno32_tset_res)))\n",
    "outF.write('\\n')\n",
    "outF.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "## Learn Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(LogisticRegression())\n",
    "scaler.fit(X_cdcl2_train)\n",
    "X_train_sds = scaler.transform(X_cdcl2_train)\n",
    "X_test_sds = scaler.transform(X_cdcl2_test)\n",
    "y_train = y_cdcl2_train\n",
    "y_test = y_cdcl2_test\n",
    "X_tset = X_cdcl2_tset\n",
    "y_tset = y_cdcl2_tset\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[104   0   0   0]\n",
      " [  0  89   0   0]\n",
      " [  0   0 106   0]\n",
      " [  0   0   0 101]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 1.0\n",
      "BACC: 1.0\n",
      "F1_micro: 1.0\n",
      "F1_macro: 1.0\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precisio_micro: 1.0\n",
      "Precisio_macro: 1.0\n",
      "Recall_micro: 1.0\n",
      "Recall_macro: 1.0\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_cdcl2_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 21 100   1 378]\n",
      " [  0   1   0   0]\n",
      " [  2 149  39 310]\n",
      " [158 342   0   0]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.041\n",
      "BACC: 0.28\n",
      "F1_micro: 0.041\n",
      "F1_macro: 0.052\n",
      "AUROC_OVR: 0.483\n",
      "AUROC_OVO: 0.437\n",
      "Precisio_micro: 0.041\n",
      "Precisio_macro: 0.273\n",
      "Recall_micro: 0.041\n",
      "Recall_macro: 0.28\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset)\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_cdcl2_tset_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pbcl2 Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(LogisticRegression())\n",
    "scaler.fit(X_cdcl2_train)\n",
    "X_train_sds = scaler.transform(X_pbcl2_train)\n",
    "X_test_sds = scaler.transform(X_pbcl2_test)\n",
    "y_train = y_pbcl2_train\n",
    "y_test = y_pbcl2_test\n",
    "X_tset = X_pbcl2_tset\n",
    "y_tset = y_pbcl2_tset\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 89   0   0   0]\n",
      " [  0 100   1   0]\n",
      " [  0   0 106   0]\n",
      " [  0   0   0 104]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.998\n",
      "BACC: 0.998\n",
      "F1_micro: 0.998\n",
      "F1_macro: 0.998\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precisio_micro: 0.998\n",
      "Precisio_macro: 0.998\n",
      "Recall_micro: 0.998\n",
      "Recall_macro: 0.998\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_pbcl2_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[  1   0   0   0]\n",
      " [327 173   0   0]\n",
      " [  0   0 500   0]\n",
      " [  0   0   0 500]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.782\n",
      "BACC: 0.836\n",
      "F1_micro: 0.782\n",
      "F1_macro: 0.63\n",
      "AUROC_OVR: 0.996\n",
      "AUROC_OVO: 0.998\n",
      "Precisio_micro: 0.782\n",
      "Precisio_macro: 0.751\n",
      "Recall_micro: 0.782\n",
      "Recall_macro: 0.836\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset)\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_pbcl2_tset_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "## Learn Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(LogisticRegression())\n",
    "scaler.fit(X_pbno32_train)\n",
    "X_train_sds = scaler.transform(X_pbno32_train)\n",
    "X_test_sds = scaler.transform(X_pbno32_test)\n",
    "y_train = y_pbno32_train\n",
    "y_test = y_pbno32_test\n",
    "X_tset = X_pbno32_tset\n",
    "y_tset = y_pbno32_tset\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 89   0   0   0]\n",
      " [  0 104   0   0]\n",
      " [  2   0 104   0]\n",
      " [  0   0   0 101]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.995\n",
      "BACC: 0.995\n",
      "F1_micro: 0.995\n",
      "F1_macro: 0.995\n",
      "AUROC_OVR: 0.999\n",
      "AUROC_OVO: 0.999\n",
      "Precisio_micro: 0.995\n",
      "Precisio_macro: 0.995\n",
      "Recall_micro: 0.995\n",
      "Recall_macro: 0.995\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_pbno32_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[  1   0   0   0]\n",
      " [468  30   2   0]\n",
      " [  0 389 111   0]\n",
      " [  0   0 436  64]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.137\n",
      "BACC: 0.353\n",
      "F1_micro: 0.137\n",
      "F1_macro: 0.127\n",
      "AUROC_OVR: 0.545\n",
      "AUROC_OVO: 0.609\n",
      "Precisio_micro: 0.137\n",
      "Precisio_macro: 0.319\n",
      "Recall_micro: 0.137\n",
      "Recall_macro: 0.353\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset)\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_pbno32_tset_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "outF = open(fileout, \"a\")\n",
    "outF.write(\"Logistic_Regression, \")\n",
    "outF.write(\"ACC, BACC, F1_micro, F1_macro, AUROC_OVR, AUROC_OVO, Precision_micro, Precision_macro, Recall_micro, Recall_macro\\n\")\n",
    "outF.write('Cdcl2 DevSet, ')\n",
    "outF.write(', '.join(map(str, LR_cdcl2_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Cdcl2 IndSet, ')\n",
    "outF.write(', '.join(map(str, LR_cdcl2_tset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pbcl2 DevSet, ')\n",
    "outF.write(', '.join(map(str, LR_pbcl2_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pbcl2 IndSet, ')\n",
    "outF.write(', '.join(map(str, LR_pbcl2_tset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pb(No3)2 DevSet, ')\n",
    "outF.write(', '.join(map(str, LR_pbno32_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pb(No3)2 IndSet, ')\n",
    "outF.write(', '.join(map(str, LR_pbno32_tset_res)))\n",
    "outF.write('\\n')\n",
    "outF.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Logistic Regression with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "## Learn Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(LogisticRegressionCV())\n",
    "scaler.fit(X_cdcl2_train)\n",
    "X_train_sds = scaler.transform(X_cdcl2_train)\n",
    "X_test_sds = scaler.transform(X_cdcl2_test)\n",
    "y_train = y_cdcl2_train\n",
    "y_test = y_cdcl2_test\n",
    "X_tset = X_cdcl2_tset\n",
    "y_tset = y_cdcl2_tset\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[104   0   0   0]\n",
      " [  0  89   0   0]\n",
      " [  0   0 106   0]\n",
      " [  0   0   0 101]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 1.0\n",
      "BACC: 1.0\n",
      "F1_micro: 1.0\n",
      "F1_macro: 1.0\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precisio_micro: 1.0\n",
      "Precisio_macro: 1.0\n",
      "Recall_micro: 1.0\n",
      "Recall_macro: 1.0\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_CV_cdcl2_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 21 100   0 379]\n",
      " [  0   1   0   0]\n",
      " [  2 149  40 309]\n",
      " [153 347   0   0]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.041\n",
      "BACC: 0.28\n",
      "F1_micro: 0.041\n",
      "F1_macro: 0.053\n",
      "AUROC_OVR: 0.512\n",
      "AUROC_OVO: 0.448\n",
      "Precisio_micro: 0.041\n",
      "Precisio_macro: 0.28\n",
      "Recall_micro: 0.041\n",
      "Recall_macro: 0.28\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset)\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_CV_cdcl2_tset_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pbcl2 Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(LogisticRegressionCV())\n",
    "scaler.fit(X_cdcl2_train)\n",
    "X_train_sds = scaler.transform(X_pbcl2_train)\n",
    "X_test_sds = scaler.transform(X_pbcl2_test)\n",
    "y_train = y_pbcl2_train\n",
    "y_test = y_pbcl2_test\n",
    "X_tset = X_pbcl2_tset\n",
    "y_tset = y_pbcl2_tset\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 89   0   0   0]\n",
      " [  0 100   1   0]\n",
      " [  0   0 106   0]\n",
      " [  0   0   0 104]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.998\n",
      "BACC: 0.998\n",
      "F1_micro: 0.998\n",
      "F1_macro: 0.998\n",
      "AUROC_OVR: 0.999\n",
      "AUROC_OVO: 0.999\n",
      "Precisio_micro: 0.998\n",
      "Precisio_macro: 0.998\n",
      "Recall_micro: 0.998\n",
      "Recall_macro: 0.998\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_CV_pbcl2_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[  1   0   0   0]\n",
      " [296 204   0   0]\n",
      " [  0   0 500   0]\n",
      " [  0   0  17 483]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.791\n",
      "BACC: 0.843\n",
      "F1_micro: 0.791\n",
      "F1_macro: 0.638\n",
      "AUROC_OVR: 0.999\n",
      "AUROC_OVO: 1.0\n",
      "Precisio_micro: 0.791\n",
      "Precisio_macro: 0.743\n",
      "Recall_micro: 0.791\n",
      "Recall_macro: 0.843\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset)\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_CV_pbcl2_tset_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "## Learn Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(LogisticRegressionCV())\n",
    "scaler.fit(X_pbno32_train)\n",
    "X_train_sds = scaler.transform(X_pbno32_train)\n",
    "X_test_sds = scaler.transform(X_pbno32_test)\n",
    "y_train = y_pbno32_train\n",
    "y_test = y_pbno32_test\n",
    "X_tset = X_pbno32_tset\n",
    "y_tset = y_pbno32_tset\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 89   0   0   0]\n",
      " [  0 103   0   1]\n",
      " [  2   0 104   0]\n",
      " [  0   0   0 101]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.992\n",
      "BACC: 0.993\n",
      "F1_micro: 0.992\n",
      "F1_macro: 0.992\n",
      "AUROC_OVR: 0.999\n",
      "AUROC_OVO: 0.999\n",
      "Precisio_micro: 0.992\n",
      "Precisio_macro: 0.992\n",
      "Recall_micro: 0.992\n",
      "Recall_macro: 0.993\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_CV_pbno32_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[  1   0   0   0]\n",
      " [433  58   9   0]\n",
      " [  0 384 116   0]\n",
      " [  0   0 440  60]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.157\n",
      "BACC: 0.367\n",
      "F1_micro: 0.157\n",
      "F1_macro: 0.14\n",
      "AUROC_OVR: 0.58\n",
      "AUROC_OVO: 0.641\n",
      "Precisio_micro: 0.157\n",
      "Precisio_macro: 0.335\n",
      "Recall_micro: 0.157\n",
      "Recall_macro: 0.367\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset)\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_CV_pbno32_tset_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "outF = open(fileout, \"a\")\n",
    "outF.write(\"Logistic_Regression_CV, \")\n",
    "outF.write(\"ACC, BACC, F1_micro, F1_macro, AUROC_OVR, AUROC_OVO, Precision_micro, Precision_macro, Recall_micro, Recall_macro\\n\")\n",
    "outF.write('Cdcl2 DevSet, ')\n",
    "outF.write(', '.join(map(str, LR_CV_cdcl2_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Cdcl2 IndSet, ')\n",
    "outF.write(', '.join(map(str, LR_CV_cdcl2_tset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pbcl2 DevSet, ')\n",
    "outF.write(', '.join(map(str, LR_CV_pbcl2_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pbcl2 IndSet, ')\n",
    "outF.write(', '.join(map(str, LR_CV_pbcl2_tset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pb(No3)2 DevSet, ')\n",
    "outF.write(', '.join(map(str, LR_CV_pbno32_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pb(No3)2 IndSet, ')\n",
    "outF.write(', '.join(map(str, LR_CV_pbno32_tset_res)))\n",
    "outF.write('\\n')\n",
    "outF.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learn Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(MLPClassifier())\n",
    "scaler.fit(X_cdcl2_train)\n",
    "X_train_sds = scaler.transform(X_cdcl2_train)\n",
    "X_test_sds = scaler.transform(X_cdcl2_test)\n",
    "y_train = y_cdcl2_train\n",
    "y_test = y_cdcl2_test\n",
    "X_tset = X_cdcl2_tset\n",
    "y_tset = y_cdcl2_tset\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[103   0   0   1]\n",
      " [  0  89   0   0]\n",
      " [  0   0 106   0]\n",
      " [  0   0   0 101]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.998\n",
      "BACC: 0.998\n",
      "F1_micro: 0.998\n",
      "F1_macro: 0.998\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precisio_micro: 0.998\n",
      "Precisio_macro: 0.998\n",
      "Recall_micro: 0.998\n",
      "Recall_macro: 0.998\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_cdcl2_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 22  87   8 383]\n",
      " [  0   1   0   0]\n",
      " [  1  35 115 349]\n",
      " [  2 497   1   0]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.092\n",
      "BACC: 0.318\n",
      "F1_micro: 0.092\n",
      "F1_macro: 0.114\n",
      "AUROC_OVR: 0.444\n",
      "AUROC_OVO: 0.416\n",
      "Precisio_micro: 0.092\n",
      "Precisio_macro: 0.452\n",
      "Recall_micro: 0.092\n",
      "Recall_macro: 0.318\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset)\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_cdcl2_tset_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pbcl2 Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(MLPClassifier())\n",
    "scaler.fit(X_cdcl2_train)\n",
    "X_train_sds = scaler.transform(X_pbcl2_train)\n",
    "X_test_sds = scaler.transform(X_pbcl2_test)\n",
    "y_train = y_pbcl2_train\n",
    "y_test = y_pbcl2_test\n",
    "X_tset = X_pbcl2_tset\n",
    "y_tset = y_pbcl2_tset\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 89   0   0   0]\n",
      " [  0 100   1   0]\n",
      " [  0   0 106   0]\n",
      " [  0   0   0 104]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.998\n",
      "BACC: 0.998\n",
      "F1_micro: 0.998\n",
      "F1_macro: 0.998\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precisio_micro: 0.998\n",
      "Precisio_macro: 0.998\n",
      "Recall_micro: 0.998\n",
      "Recall_macro: 0.998\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_pbcl2_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[  1   0   0   0]\n",
      " [268 232   0   0]\n",
      " [ 40  54 406   0]\n",
      " [  0   0   0 500]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.759\n",
      "BACC: 0.819\n",
      "F1_micro: 0.759\n",
      "F1_macro: 0.623\n",
      "AUROC_OVR: 0.977\n",
      "AUROC_OVO: 0.984\n",
      "Precisio_micro: 0.759\n",
      "Precisio_macro: 0.704\n",
      "Recall_micro: 0.759\n",
      "Recall_macro: 0.819\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset)\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_pbcl2_tset_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learn Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(MLPClassifier())\n",
    "scaler.fit(X_pbno32_train)\n",
    "X_train_sds = scaler.transform(X_pbno32_train)\n",
    "X_test_sds = scaler.transform(X_pbno32_test)\n",
    "y_train = y_pbno32_train\n",
    "y_test = y_pbno32_test\n",
    "X_tset = X_pbno32_tset\n",
    "y_tset = y_pbno32_tset\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 89   0   0   0]\n",
      " [  0 104   0   0]\n",
      " [  1   0 105   0]\n",
      " [  0   0   0 101]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.998\n",
      "BACC: 0.998\n",
      "F1_micro: 0.998\n",
      "F1_macro: 0.997\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precisio_micro: 0.998\n",
      "Precisio_macro: 0.997\n",
      "Recall_micro: 0.998\n",
      "Recall_macro: 0.998\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_pbno32_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[  1   0   0   0]\n",
      " [318  12  70 100]\n",
      " [ 25 384  87   4]\n",
      " [  0  17  20 463]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.375\n",
      "BACC: 0.531\n",
      "F1_micro: 0.375\n",
      "F1_macro: 0.289\n",
      "AUROC_OVR: 0.651\n",
      "AUROC_OVO: 0.691\n",
      "Precisio_micro: 0.375\n",
      "Precisio_macro: 0.335\n",
      "Recall_micro: 0.375\n",
      "Recall_macro: 0.531\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset)\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_pbno32_tset_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "outF = open(fileout, \"a\")\n",
    "outF.write(\"MLP, \")\n",
    "outF.write(\"ACC, BACC, F1_micro, F1_macro, AUROC_OVR, AUROC_OVO, Precision_micro, Precision_macro, Recall_micro, Recall_macro\\n\")\n",
    "outF.write('Cdcl2 DevSet, ')\n",
    "outF.write(', '.join(map(str, MLP_cdcl2_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Cdcl2 IndSet, ')\n",
    "outF.write(', '.join(map(str, MLP_cdcl2_tset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pbcl2 DevSet, ')\n",
    "outF.write(', '.join(map(str, MLP_pbcl2_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pbcl2 IndSet, ')\n",
    "outF.write(', '.join(map(str, MLP_pbcl2_tset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pb(No3)2 DevSet, ')\n",
    "outF.write(', '.join(map(str, MLP_pbno32_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pb(No3)2 IndSet, ')\n",
    "outF.write(', '.join(map(str, MLP_pbno32_tset_res)))\n",
    "outF.write('\\n')\n",
    "outF.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learn Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(RandomForestClassifier())\n",
    "scaler.fit(X_cdcl2_train)\n",
    "X_train_sds = scaler.transform(X_cdcl2_train)\n",
    "X_test_sds = scaler.transform(X_cdcl2_test)\n",
    "y_train = y_cdcl2_train\n",
    "y_test = y_cdcl2_test\n",
    "X_tset = X_cdcl2_tset\n",
    "y_tset = y_cdcl2_tset\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[103   0   0   1]\n",
      " [  0  89   0   0]\n",
      " [  0   1 103   2]\n",
      " [  0   0   0 101]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.99\n",
      "BACC: 0.991\n",
      "F1_micro: 0.99\n",
      "F1_macro: 0.99\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precisio_micro: 0.99\n",
      "Precisio_macro: 0.99\n",
      "Recall_micro: 0.99\n",
      "Recall_macro: 0.991\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_cdcl2_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[  3  37   1 459]\n",
      " [  0   1   0   0]\n",
      " [109  11   0 380]\n",
      " [  8   2   0 490]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.329\n",
      "BACC: 0.496\n",
      "F1_micro: 0.329\n",
      "F1_macro: 0.146\n",
      "AUROC_OVR: 0.64\n",
      "AUROC_OVO: 0.692\n",
      "Precisio_micro: 0.329\n",
      "Precisio_macro: 0.103\n",
      "Recall_micro: 0.329\n",
      "Recall_macro: 0.496\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset)\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_cdcl2_tset_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pbcl2 Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(RandomForestClassifier())\n",
    "scaler.fit(X_cdcl2_train)\n",
    "X_train_sds = scaler.transform(X_pbcl2_train)\n",
    "X_test_sds = scaler.transform(X_pbcl2_test)\n",
    "y_train = y_pbcl2_train\n",
    "y_test = y_pbcl2_test\n",
    "X_tset = X_pbcl2_tset\n",
    "y_tset = y_pbcl2_tset\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 89   0   0   0]\n",
      " [  0 100   1   0]\n",
      " [  0   0 106   0]\n",
      " [  0   0   0 104]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.998\n",
      "BACC: 0.998\n",
      "F1_micro: 0.998\n",
      "F1_macro: 0.998\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precisio_micro: 0.998\n",
      "Precisio_macro: 0.998\n",
      "Recall_micro: 0.998\n",
      "Recall_macro: 0.998\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_pbcl2_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[  1   0   0   0]\n",
      " [332 168   0   0]\n",
      " [ 46  18 436   0]\n",
      " [  0   0   1 499]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.736\n",
      "BACC: 0.802\n",
      "F1_micro: 0.736\n",
      "F1_macro: 0.606\n",
      "AUROC_OVR: 0.99\n",
      "AUROC_OVO: 0.993\n",
      "Precisio_micro: 0.736\n",
      "Precisio_macro: 0.726\n",
      "Recall_micro: 0.736\n",
      "Recall_macro: 0.802\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset)\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_pbcl2_tset_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learn Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(RandomForestClassifier())\n",
    "scaler.fit(X_pbno32_train)\n",
    "X_train_sds = scaler.transform(X_pbno32_train)\n",
    "X_test_sds = scaler.transform(X_pbno32_test)\n",
    "y_train = y_pbno32_train\n",
    "y_test = y_pbno32_test\n",
    "X_tset = X_pbno32_tset\n",
    "y_tset = y_pbno32_tset\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 89   0   0   0]\n",
      " [  0 103   0   1]\n",
      " [  1   0 105   0]\n",
      " [  0   0   0 101]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.995\n",
      "BACC: 0.995\n",
      "F1_micro: 0.995\n",
      "F1_macro: 0.995\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precisio_micro: 0.995\n",
      "Precisio_macro: 0.995\n",
      "Recall_micro: 0.995\n",
      "Recall_macro: 0.995\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_pbno32_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[  1   0   0   0]\n",
      " [380  39  49  32]\n",
      " [  2 111 387   0]\n",
      " [  0   0 267 233]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.44\n",
      "BACC: 0.58\n",
      "F1_micro: 0.44\n",
      "F1_macro: 0.344\n",
      "AUROC_OVR: 0.739\n",
      "AUROC_OVO: 0.784\n",
      "Precisio_micro: 0.44\n",
      "Precisio_macro: 0.423\n",
      "Recall_micro: 0.44\n",
      "Recall_macro: 0.58\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset)\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_pbno32_tset_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "outF = open(fileout, \"a\")\n",
    "outF.write(\"Random_Forest, \")\n",
    "outF.write(\"ACC, BACC, F1_micro, F1_macro, AUROC_OVR, AUROC_OVO, Precision_micro, Precision_macro, Recall_micro, Recall_macro\\n\")\n",
    "outF.write('Cdcl2 DevSet, ')\n",
    "outF.write(', '.join(map(str, RF_cdcl2_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Cdcl2 IndSet, ')\n",
    "outF.write(', '.join(map(str, RF_cdcl2_tset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pbcl2 DevSet, ')\n",
    "outF.write(', '.join(map(str, RF_pbcl2_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pbcl2 IndSet, ')\n",
    "outF.write(', '.join(map(str, RF_pbcl2_tset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pb(No3)2 DevSet, ')\n",
    "outF.write(', '.join(map(str, RF_pbno32_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pb(No3)2 IndSet, ')\n",
    "outF.write(', '.join(map(str, RF_pbno32_tset_res)))\n",
    "outF.write('\\n')\n",
    "outF.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learn Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(SVC(kernel = 'linear', probability=True))\n",
    "scaler.fit(X_cdcl2_train)\n",
    "X_train_sds = scaler.transform(X_cdcl2_train)\n",
    "X_test_sds = scaler.transform(X_cdcl2_test)\n",
    "y_train = y_cdcl2_train\n",
    "y_test = y_cdcl2_test\n",
    "X_tset = X_cdcl2_tset\n",
    "y_tset = y_cdcl2_tset\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[104   0   0   0]\n",
      " [  0  89   0   0]\n",
      " [  0   0 106   0]\n",
      " [  0   0   0 101]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 1.0\n",
      "BACC: 1.0\n",
      "F1_micro: 1.0\n",
      "F1_macro: 1.0\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precisio_micro: 1.0\n",
      "Precisio_macro: 1.0\n",
      "Recall_micro: 1.0\n",
      "Recall_macro: 1.0\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinSVM_cdcl2_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 18 100   0 382]\n",
      " [  0   1   0   0]\n",
      " [  4 149  65 282]\n",
      " [129 371   0   0]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.056\n",
      "BACC: 0.292\n",
      "F1_micro: 0.056\n",
      "F1_macro: 0.072\n",
      "AUROC_OVR: 0.537\n",
      "AUROC_OVO: 0.481\n",
      "Precisio_micro: 0.056\n",
      "Precisio_macro: 0.28\n",
      "Recall_micro: 0.056\n",
      "Recall_macro: 0.292\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset)\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinSVM_cdcl2_tset_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pbcl2 Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(SVC(kernel = 'linear', probability=True))\n",
    "scaler.fit(X_cdcl2_train)\n",
    "X_train_sds = scaler.transform(X_pbcl2_train)\n",
    "X_test_sds = scaler.transform(X_pbcl2_test)\n",
    "y_train = y_pbcl2_train\n",
    "y_test = y_pbcl2_test\n",
    "X_tset = X_pbcl2_tset\n",
    "y_tset = y_pbcl2_tset\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 89   0   0   0]\n",
      " [  0 100   1   0]\n",
      " [  0   0 106   0]\n",
      " [  0   0   0 104]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.998\n",
      "BACC: 0.998\n",
      "F1_micro: 0.998\n",
      "F1_macro: 0.998\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precisio_micro: 0.998\n",
      "Precisio_macro: 0.998\n",
      "Recall_micro: 0.998\n",
      "Recall_macro: 0.998\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinSVM_pbcl2_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[  1   0   0   0]\n",
      " [301 199   0   0]\n",
      " [  0   0 500   0]\n",
      " [  0   0   0 500]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.799\n",
      "BACC: 0.85\n",
      "F1_micro: 0.799\n",
      "F1_macro: 0.644\n",
      "AUROC_OVR: 0.999\n",
      "AUROC_OVO: 0.999\n",
      "Precisio_micro: 0.799\n",
      "Precisio_macro: 0.751\n",
      "Recall_micro: 0.799\n",
      "Recall_macro: 0.85\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset)\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinSVM_pbcl2_tset_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learn Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(SVC(kernel = 'linear', probability=True))\n",
    "scaler.fit(X_pbno32_train)\n",
    "X_train_sds = scaler.transform(X_pbno32_train)\n",
    "X_test_sds = scaler.transform(X_pbno32_test)\n",
    "y_train = y_pbno32_train\n",
    "y_test = y_pbno32_test\n",
    "X_tset = X_pbno32_tset\n",
    "y_tset = y_pbno32_tset\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 89   0   0   0]\n",
      " [  0 104   0   0]\n",
      " [  1   0 105   0]\n",
      " [  0   0   0 101]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.998\n",
      "BACC: 0.998\n",
      "F1_micro: 0.998\n",
      "F1_macro: 0.997\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precisio_micro: 0.998\n",
      "Precisio_macro: 0.997\n",
      "Recall_micro: 0.998\n",
      "Recall_macro: 0.998\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinSVM_pbno32_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[  1   0   0   0]\n",
      " [388  97  15   0]\n",
      " [  0 373 127   0]\n",
      " [  0   0 455  45]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.18\n",
      "BACC: 0.384\n",
      "F1_micro: 0.18\n",
      "F1_macro: 0.15\n",
      "AUROC_OVR: 0.609\n",
      "AUROC_OVO: 0.677\n",
      "Precisio_micro: 0.18\n",
      "Precisio_macro: 0.355\n",
      "Recall_micro: 0.18\n",
      "Recall_macro: 0.384\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset)\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinSVM_pbno32_tset_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "outF = open(fileout, \"a\")\n",
    "outF.write(\"Linear_SVM, \")\n",
    "outF.write(\"ACC, BACC, F1_micro, F1_macro, AUROC_OVR, AUROC_OVO, Precision_micro, Precision_macro, Recall_micro, Recall_macro\\n\")\n",
    "outF.write('Cdcl2 DevSet, ')\n",
    "outF.write(', '.join(map(str, LinSVM_cdcl2_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Cdcl2 IndSet, ')\n",
    "outF.write(', '.join(map(str, LinSVM_cdcl2_tset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pbcl2 DevSet, ')\n",
    "outF.write(', '.join(map(str, LinSVM_pbcl2_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pbcl2 IndSet, ')\n",
    "outF.write(', '.join(map(str, LinSVM_pbcl2_tset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pb(No3)2 DevSet, ')\n",
    "outF.write(', '.join(map(str, LinSVM_pbno32_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pb(No3)2 IndSet, ')\n",
    "outF.write(', '.join(map(str, LinSVM_pbno32_tset_res)))\n",
    "outF.write('\\n')\n",
    "outF.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 RBF SVM ( Nonlinear SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learn Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(SVC(kernel = 'rbf', probability=True))\n",
    "scaler.fit(X_cdcl2_train)\n",
    "X_train_sds = scaler.transform(X_cdcl2_train)\n",
    "X_test_sds = scaler.transform(X_cdcl2_test)\n",
    "y_train = y_cdcl2_train\n",
    "y_test = y_cdcl2_test\n",
    "X_tset = X_cdcl2_tset\n",
    "y_tset = y_cdcl2_tset\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[103   0   0   1]\n",
      " [  1  88   0   0]\n",
      " [  0   3 103   0]\n",
      " [  0   1   0 100]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.985\n",
      "BACC: 0.985\n",
      "F1_micro: 0.985\n",
      "F1_macro: 0.985\n",
      "AUROC_OVR: 0.999\n",
      "AUROC_OVO: 0.999\n",
      "Precisio_micro: 0.985\n",
      "Precisio_macro: 0.984\n",
      "Recall_micro: 0.985\n",
      "Recall_macro: 0.985\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "RBFSVM_cdcl2_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 19  96   1 384]\n",
      " [  0   1   0   0]\n",
      " [  1 150   0 349]\n",
      " [  9 491   0   0]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.013\n",
      "BACC: 0.26\n",
      "F1_micro: 0.013\n",
      "F1_macro: 0.019\n",
      "AUROC_OVR: 0.439\n",
      "AUROC_OVO: 0.405\n",
      "Precisio_micro: 0.013\n",
      "Precisio_macro: 0.164\n",
      "Recall_micro: 0.013\n",
      "Recall_macro: 0.26\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset)\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "RBFSVM_cdcl2_tset_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pbcl2 Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(SVC(kernel = 'rbf', probability=True))\n",
    "scaler.fit(X_cdcl2_train)\n",
    "X_train_sds = scaler.transform(X_pbcl2_train)\n",
    "X_test_sds = scaler.transform(X_pbcl2_test)\n",
    "y_train = y_pbcl2_train\n",
    "y_test = y_pbcl2_test\n",
    "X_tset = X_pbcl2_tset\n",
    "y_tset = y_pbcl2_tset\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 89   0   0   0]\n",
      " [  0 100   1   0]\n",
      " [  0   0 106   0]\n",
      " [  0   0   0 104]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.998\n",
      "BACC: 0.998\n",
      "F1_micro: 0.998\n",
      "F1_macro: 0.998\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precisio_micro: 0.998\n",
      "Precisio_macro: 0.998\n",
      "Recall_micro: 0.998\n",
      "Recall_macro: 0.998\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "RBFSVM_pbcl2_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[  1   0   0   0]\n",
      " [334 166   0   0]\n",
      " [  0   0 500   0]\n",
      " [  0   0   0 500]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.777\n",
      "BACC: 0.833\n",
      "F1_micro: 0.777\n",
      "F1_macro: 0.626\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precisio_micro: 0.777\n",
      "Precisio_macro: 0.751\n",
      "Recall_micro: 0.777\n",
      "Recall_macro: 0.833\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset)\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "RBFSVM_pbcl2_tset_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learn Classifier\n",
    "scaler = StandardScaler()\n",
    "clf = OrdinalClassifier(SVC(kernel = 'rbf', probability=True))\n",
    "scaler.fit(X_pbno32_train)\n",
    "X_train_sds = scaler.transform(X_pbno32_train)\n",
    "X_test_sds = scaler.transform(X_pbno32_test)\n",
    "y_train = y_pbno32_train\n",
    "y_test = y_pbno32_test\n",
    "X_tset = X_pbno32_tset\n",
    "y_tset = y_pbno32_tset\n",
    "clf.fit(X_train_sds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[ 89   0   0   0]\n",
      " [  0 103   1   0]\n",
      " [  1   0 105   0]\n",
      " [  0   0   0 101]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.995\n",
      "BACC: 0.995\n",
      "F1_micro: 0.995\n",
      "F1_macro: 0.995\n",
      "AUROC_OVR: 1.0\n",
      "AUROC_OVO: 1.0\n",
      "Precisio_micro: 0.995\n",
      "Precisio_macro: 0.995\n",
      "Recall_micro: 0.995\n",
      "Recall_macro: 0.995\n"
     ]
    }
   ],
   "source": [
    "### Test within batch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "yp_test = clf.predict(X_test_sds)\n",
    "ys_test = clf.predict_proba(X_test_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_test, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_test, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_test, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_test, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_test, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_test, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_test, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_test, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "RBFSVM_pbno32_devset_res = [round(accuracy_score(y_test, yp_test), 3), round(balanced_accuracy_score(y_test, yp_test), 3),\n",
    "                 round(f1_score(y_test, yp_test, average='micro'),3), round(f1_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_test, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_test, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_test, yp_test, average='micro'),3), round(precision_score(y_test, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_test, yp_test, average='micro'),3), round(recall_score(y_test, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConfusion Matrix\u001b[0m\n",
      "[[  1   0   0   0]\n",
      " [249  21 230   0]\n",
      " [  1 201 298   0]\n",
      " [  0   0 448  52]]\n",
      "\u001b[1mMetrics\u001b[0m\n",
      "ACC: 0.248\n",
      "BACC: 0.436\n",
      "F1_micro: 0.248\n",
      "F1_macro: 0.165\n",
      "AUROC_OVR: 0.709\n",
      "AUROC_OVO: 0.786\n",
      "Precisio_micro: 0.248\n",
      "Precisio_macro: 0.351\n",
      "Recall_micro: 0.248\n",
      "Recall_macro: 0.436\n"
     ]
    }
   ],
   "source": [
    "### Independent Test Result\n",
    "X_tset_sds = scaler.transform(X_tset)\n",
    "yp_test = clf.predict(X_tset_sds)\n",
    "ys_test = clf.predict_proba(X_tset_sds)\n",
    "\n",
    "print('\\033[1m' + 'Confusion Matrix' + '\\033[0m')\n",
    "print(confusion_matrix(y_tset, yp_test))\n",
    "\n",
    "print('\\033[1m' + 'Metrics' + '\\033[0m')\n",
    "print('ACC: {}'.format(round(accuracy_score(y_tset, yp_test), 3)))\n",
    "print('BACC: {}'.format(round(balanced_accuracy_score(y_tset, yp_test), 3)))\n",
    "print('F1_micro: {}'.format(round(f1_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('F1_macro: {}'.format(round(f1_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('AUROC_OVR: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3)))\n",
    "print('AUROC_OVO: {}'.format(round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3)))\n",
    "print('Precisio_micro: {}'.format(round(precision_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Precisio_macro: {}'.format(round(precision_score(y_tset, yp_test, average='macro'),3)))\n",
    "print('Recall_micro: {}'.format(round(recall_score(y_tset, yp_test, average='micro'),3)))\n",
    "print('Recall_macro: {}'.format(round(recall_score(y_tset, yp_test, average='macro'),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "RBFSVM_pbno32_tset_res = [round(accuracy_score(y_tset, yp_test), 3), round(balanced_accuracy_score(y_tset, yp_test), 3),\n",
    "                 round(f1_score(y_tset, yp_test, average='micro'),3), round(f1_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(roc_auc_score(y_tset, ys_test, multi_class='ovr'),3), round(roc_auc_score(y_tset, ys_test, multi_class='ovo'),3),\n",
    "                 round(precision_score(y_tset, yp_test, average='micro'),3), round(precision_score(y_tset, yp_test, average='macro'),3),\n",
    "                 round(recall_score(y_tset, yp_test, average='micro'),3), round(recall_score(y_tset, yp_test, average='macro'),3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "outF = open(fileout, \"a\")\n",
    "outF.write(\"RBF_SVM, \")\n",
    "outF.write(\"ACC, BACC, F1_micro, F1_macro, AUROC_OVR, AUROC_OVO, Precision_micro, Precision_macro, Recall_micro, Recall_macro\\n\")\n",
    "outF.write('Cdcl2 DevSet, ')\n",
    "outF.write(', '.join(map(str, RBFSVM_cdcl2_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Cdcl2 IndSet, ')\n",
    "outF.write(', '.join(map(str, RBFSVM_cdcl2_tset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pbcl2 DevSet, ')\n",
    "outF.write(', '.join(map(str, RBFSVM_pbcl2_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pbcl2 IndSet, ')\n",
    "outF.write(', '.join(map(str, RBFSVM_pbcl2_tset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pb(No3)2 DevSet, ')\n",
    "outF.write(', '.join(map(str, RBFSVM_pbno32_devset_res)))\n",
    "outF.write('\\n')\n",
    "outF.write('Pb(No3)2 IndSet, ')\n",
    "outF.write(', '.join(map(str, RBFSVM_pbno32_tset_res)))\n",
    "outF.write('\\n')\n",
    "outF.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
